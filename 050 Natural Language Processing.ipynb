{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing\n",
    "Die Verarbeitung von natürlicher Sprache ist eine der größten Herausforderungen in der Informatik.\n",
    "\n",
    "### Bibliotheken:\n",
    "- NLTK\n",
    "- PyTorch NLP\n",
    "\n",
    "### Seiten:\n",
    "- https://nlpforhackers.io\n",
    "\n",
    "\n",
    "### Installation:\n",
    "Neben der einfachen Installation von nltk über **pip install nltk**, benötigt die Bibliothek häufig sogenannte Corpora. Sollte ein solcher fehlen, wird entsprechend in der Fehlermeldung darauf hingewiesen. mit **nltk.download()** im Terminal wird das Download-Fenster von nltk geöffnet und die fehlenden Dateien nachgeladen. Im Beispiel-Code wird die Bibliothek per Befehl runtergeladen (_nltk.download('punkt')_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /root/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Falls ein Proxy genutzt wird\n",
    "# nltk.set_proxy('http://proxy.example.com:3128', ('USERNAME', 'PASSWORD'))\n",
    "\n",
    "# Herunterladen aller notwendigen Corpora\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization zum Unterteilen von Texten\n",
    "Bevor ein Text verarbeitet wird, wird er über einen Tokenizer in kleinere Einheiten unterteilt, etwa Sätze und Wörter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Satzebene:\n",
      "['Cyber Security is a challenge for governments and industries.']\n",
      "\n",
      "Wortebene:\n",
      "['Cyber', 'Security', 'is', 'a', 'challenge', 'for', 'governments', 'and', 'industries', '.']\n"
     ]
    }
   ],
   "source": [
    "# German corpus\n",
    "# https://datascience.blog.wzb.eu/2016/07/13/accurate-part-of-speech-tagging-of-german-texts-with-nltk/\n",
    "\n",
    "text = \"Cyber Security is a challenge for governments and industries.\"\n",
    "\n",
    "# Tokenizer zum Unterteilen von Texten in einzelne Sätze\n",
    "sentences = nltk.tokenize.sent_tokenize(text)\n",
    "print(\"Satzebene:\")\n",
    "print(sentences)\n",
    "\n",
    "# Tokenizer zum Unterteilen von Texten in einzelne Wörter.\n",
    "tokens = word_tokenize(text)\n",
    "print(\"\\nWortebene:\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming zum Normalisieren von Wörtern\n",
    "Sollte es nötig sein Wörter vor der Verarbeitung zu normalisieren, trifft man häufig auf den Begriff Stemmer. Dieser wandet u.a. Mehrzahlbegriffe in Einzahlbegriffe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter result:\n",
      "['cyber', 'secur', 'is', 'a', 'challeng', 'for', 'govern', 'and', 'industri', '.']\n",
      "Snowball result:\n",
      "['cyber', 'secur', 'is', 'a', 'challeng', 'for', 'govern', 'and', 'industri', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# porter stemmer\n",
    "stemmer_porter = PorterStemmer()\n",
    "normalized_words_porter = [stemmer_porter.stem(token) for token in tokens]\n",
    "\n",
    "print(\"Porter result:\")\n",
    "print(normalized_words_porter)\n",
    "\n",
    "# snowball stemmer\n",
    "stemmer_snowball = SnowballStemmer(\"english\", ignore_stopwords=True)\n",
    "normlized_words_snowball = [stemmer_snowball.stem(token) for token in tokens]\n",
    "\n",
    "print(\"Snowball result:\")\n",
    "print(normlized_words_snowball)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatizer für Wortnormalisierungen\n",
    "Ein besseres Ergebnis als das Stemming liefert ein Lemmatizer auf Basis einer Wortliste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatizer WordNet results:\n",
      "['Cyber', 'Security', 'is', 'a', 'challenge', 'for', 'government', 'and', 'industry', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer_wordnet = WordNetLemmatizer()\n",
    "normlized_words_wordnet = [lemmatizer_wordnet.lemmatize(token) for token in tokens]\n",
    "\n",
    "print(\"Lemmatizer WordNet results:\")\n",
    "print(normlized_words_wordnet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part of Speech (POS) Tagging\n",
    "Beim POS-tagging werden die Klassen einzelner Wörter im Satz erfasst, also z.B. Nomen, Verben und Präpositionen. Die Ausgabe mag etwas kryptisch erscheinen, jedoch  bietet _nltk_ mehrere Funktionen, um mit dem Ergebnis zu arbeiten. Diese sollen im späteren Verlauf betrachtet werden.\n",
    "\n",
    "Übliche Tags beim Part of Speech Tagging sind hier zu sehen (Quelle: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)\n",
    "\n",
    "<table>\n",
    "\t<tr>\n",
    "\t\t<td><b>Nummer</b></td>\n",
    "\t\t<td><b>Tag</b></td>\n",
    "\t\t<td><b>Beschreibung</b></td>\n",
    "        <td><b>Nummer</b></td>\n",
    "\t\t<td><b>Tag</b></td>\n",
    "\t\t<td><b>Beschreibung</b></td>\n",
    "\t</tr>\n",
    "  <tr> \n",
    "    <td> 1. </td>\n",
    "    <td>CC </td>\n",
    "    <td>Coordinating conjunction </td>\n",
    "    <td> 2. </td>\n",
    "    <td>CD </td>\n",
    "    <td>Cardinal number </td>\n",
    "  </tr>\n",
    "  <tr> \n",
    "    <td> 3. </td>\n",
    "    <td>DT </td>\n",
    "    <td>Determiner </td>\n",
    "    <td> 4. </td>\n",
    "    <td>EX </td>\n",
    "    <td>Existential <i>there<i> </i></i></td>\n",
    "  </tr>\n",
    "  <tr> \n",
    "    <td> 5. </td>\n",
    "    <td>FW </td>\n",
    "    <td>Foreign word </td>\n",
    "    <td> 6. </td>\n",
    "    <td>IN </td>\n",
    "    <td>Preposition or subordinating conjunction </td>\n",
    "  </tr>\n",
    "  <tr> \n",
    "    <td> 7. </td>\n",
    "    <td>JJ </td>\n",
    "    <td>Adjective </td>\n",
    "    <td> 8. </td>\n",
    "    <td>JJR </td>\n",
    "    <td>Adjective, comparative </td>\n",
    "  </tr>\n",
    "  <tr> \n",
    "    <td> 9. </td>\n",
    "    <td>JJS </td>\n",
    "    <td>Adjective, superlative </td>\n",
    "    <td> 10. </td>\n",
    "    <td>LS </td>\n",
    "    <td>List item marker </td>\n",
    "  </tr>\n",
    "  <tr> \n",
    "    <td> 11. </td>\n",
    "    <td>MD </td>\n",
    "    <td>Modal </td> \n",
    "    <td> 12. </td>\n",
    "    <td>NN </td>\n",
    "    <td>Noun, singular or mass </td>\n",
    "  </tr>\n",
    "  <tr> \n",
    "    <td> 13. </td>\n",
    "    <td>NNS </td>\n",
    "    <td>Noun, plural </td>\n",
    "    <td> 14. </td>\n",
    "    <td>NNP </td>\n",
    "    <td>Proper noun, singular </td>\n",
    "  </tr>\n",
    "  <tr> \n",
    "    <td> 15. </td>\n",
    "    <td>NNPS </td>\n",
    "    <td>Proper noun, plural </td>\n",
    "    <td> 16. </td>\n",
    "    <td>PDT </td>\n",
    "    <td>Predeterminer </td>\n",
    "  </tr>\n",
    "  <tr> \n",
    "    <td> 17. </td>\n",
    "    <td>POS </td>\n",
    "    <td>Possessive ending </td>\n",
    "    <td> 18. </td>\n",
    "    <td>PRP </td>\n",
    "    <td>Personal pronoun </td>\n",
    "  </tr>\n",
    "  <tr> \n",
    "    <td> 19. </td>\n",
    "    <td>PRP&#36; </td>\n",
    "    <td>Possessive pronoun </td>\n",
    "    <td> 20. </td>\n",
    "    <td>RB </td>\n",
    "    <td>Adverb </td>\n",
    "  </tr>\n",
    "  <tr> \n",
    "    <td> 21. </td>\n",
    "    <td>RBR </td>\n",
    "    <td>Adverb, comparative </td>\n",
    "    <td> 22. </td>\n",
    "    <td>RBS </td>\n",
    "    <td>Adverb, superlative </td>\n",
    "  </tr>\n",
    "  <tr> \n",
    "    <td> 23. </td>\n",
    "    <td>RP </td>\n",
    "    <td>Particle </td>\n",
    "    <td> 24. </td>\n",
    "    <td>SYM </td>\n",
    "    <td>Symbol </td>\n",
    "  </tr>\n",
    "  <tr> \n",
    "    <td> 25. </td>\n",
    "    <td>TO </td>\n",
    "    <td><i>to</i> </td> \n",
    "    <td> 26. </td>\n",
    "    <td>UH </td>\n",
    "    <td>Interjection </td>\n",
    "  </tr>\n",
    "  <tr> \n",
    "    <td> 27. </td>\n",
    "    <td>VB </td>\n",
    "    <td>Verb, base form </td>\n",
    "    <td> 28. </td>\n",
    "    <td>VBD </td>\n",
    "    <td>Verb, past tense </td>\n",
    "  </tr>\n",
    "  <tr> \n",
    "    <td> 29. </td>\n",
    "    <td>VBG </td>\n",
    "    <td>Verb, gerund or present participle </td>\n",
    "    <td> 30. </td>\n",
    "    <td>VBN </td>\n",
    "    <td>Verb, past participle </td>\n",
    "  </tr>\n",
    "  <tr> \n",
    "    <td> 31. </td>\n",
    "    <td>VBP </td>\n",
    "    <td>Verb, non-3rd person singular present </td>\n",
    "    <td> 32. </td>\n",
    "    <td>VBZ </td>\n",
    "    <td>Verb, 3rd person singular present </td>\n",
    "  </tr>\n",
    "  <tr> \n",
    "    <td> 33. </td>\n",
    "    <td>WDT </td>\n",
    "    <td>Wh-determiner </td>\n",
    "    <td> 34. </td>\n",
    "    <td>WP </td>\n",
    "    <td>Wh-pronoun </td>\n",
    "  </tr>\n",
    "  <tr> \n",
    "    <td> 35. </td>\n",
    "    <td>WP$ </td>\n",
    "    <td>Possessive wh-pronoun </td>\n",
    "    <td> 36. </td>\n",
    "    <td>WRB </td>\n",
    "    <td>Wh-adverb</td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Cyber', 'NNP'), ('Security', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('challenge', 'NN'), ('for', 'IN'), ('governments', 'NNS'), ('and', 'CC'), ('industries', 'NNS'), ('.', '.')]\n",
      "Word: Cyber\n",
      "Wortart: NNP\n",
      "\n",
      "Word: Security\n",
      "Wortart: NNP\n",
      "\n",
      "Word: is\n",
      "Wortart: VBZ\n",
      "\n",
      "Word: a\n",
      "Wortart: DT\n",
      "\n",
      "Word: challenge\n",
      "Wortart: NN\n",
      "\n",
      "Word: for\n",
      "Wortart: IN\n",
      "\n",
      "Word: governments\n",
      "Wortart: NNS\n",
      "\n",
      "Word: and\n",
      "Wortart: CC\n",
      "\n",
      "Word: industries\n",
      "Wortart: NNS\n",
      "\n",
      "Word: .\n",
      "Wortart: .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "print(tagged_tokens)\n",
    "\n",
    "# Zugriff auf Elemente eines einzelnen Tokens\n",
    "for t in tagged_tokens:\n",
    "    print(\"Word: \" + t[0] + \"\\nWortart: \" + t[1] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Semantics Level\n",
    "#ner_tree = ne_chunk(tagged_tokens)\n",
    "#print(ner_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprachenerkennung\n",
    "Häufig müssen Sprachen normalisiert werden, bevor sie analysiert werden können, sprich in eine gleiche Sprache übersetzt werden. Da viele Werkzeuge gute Corpora für Englisch bereitstellen, bietet sich die Übersetzung ins Englische an. Um eine Übersetzung durchzuführen, muss jedoch zunächst die Quellsprache ermittelt werden.\n",
    "\n",
    "Der einfachste Ansatz ist die Bibliothek _langdetect_ zu verwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en\n",
      "de\n"
     ]
    }
   ],
   "source": [
    "from langdetect import detect\n",
    "\n",
    "print(detect(\"War doesn't show who's right, just who's left.\"))\n",
    "\n",
    "print(detect(\"Ein, zwei, drei, vier\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spracherkennung anhand von Stopwords\n",
    "\n",
    "Ein probabilistischer Ansatz berechnet die Wahrscheinlichkeiten, dass ein Text in Spracht a, b oder c geschrieben ist, anhand der Vorkommnisse sogenannter Stopwords. Stopwords sind Begriffe wie \"ich\", \"haben\", \"auf\", \"im\" ... die für eine Analyse in den meisten Fällen wenig Bedeutung haben und oft herausgefiltert werden. Der Vorteil dieses Vorgehens ist, dass er recht einfach ist und _nltk_ eine eigene Liste von Stopwords pflegt.\n",
    "\n",
    "Quelle: http://blog.alejandronolla.com/2013/05/15/detecting-text-language-with-python-and-nltk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "german\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import wordpunct_tokenize\n",
    "\n",
    "def detect_language(text):\n",
    "    languages_ratios = {}\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    words = [word.lower() for word in tokens]\n",
    "\n",
    "    # Wieviele Stopwords von Sprache language sind in text\n",
    "    for language in stopwords.fileids():\n",
    "        stopwords_set = set(stopwords.words(language))\n",
    "        words_set = set(words)\n",
    "        # Bereichne die Schnittmenge von stopwords und text\n",
    "        common_elements = words_set.intersection(stopwords_set)\n",
    "\n",
    "        # Schreibe die Anzahl der Elemente der Schnittmenge als Wert für die Sprache language\n",
    "        languages_ratios[language] = len(common_elements)\n",
    "\n",
    "    most_rated_language = max(languages_ratios, key=languages_ratios.get)\n",
    "    return most_rated_language\n",
    "\n",
    "nltk.download('stopwords')\n",
    "text = \"Sicherheit ist sehr wichtig für uns.\"\n",
    "\n",
    "print(detect_language(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
